{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmanromeo/CS_5683_Big_Data_Analytics/blob/main/GCN_Bonus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom neighborhood aggregation: As you can notice, the GCN aggregates from all its neighbors, which can be costly at times. The idea for this task is to set a maximum number of random neighbors to aggregate information from. This can be done after our data pre-processing step by simply converting some entries of the adjacency matrix to zeroes. Evaluate this model's performance against all other models. Note: This has to be done for only 2-layer models and each layer should have different random samples for neighborhood aggregation.**"
      ],
      "metadata": {
        "id": "tBezsDubSu7o"
      },
      "id": "tBezsDubSu7o"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "2ed63e88",
      "metadata": {
        "id": "2ed63e88"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "fa73fc8b",
      "metadata": {
        "id": "fa73fc8b"
      },
      "outputs": [],
      "source": [
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "54da9116",
      "metadata": {
        "id": "54da9116"
      },
      "outputs": [],
      "source": [
        "def feature_normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "4c3bc9ac",
      "metadata": {
        "id": "4c3bc9ac"
      },
      "outputs": [],
      "source": [
        "def adj_normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1)) # Sum each row\n",
        "    r_inv = np.power(rowsum, -1/2).flatten() # Negative square root\n",
        "    r_mat_inv = sp.diags(r_inv) # Create diagonal matrix\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    mx = mx.dot(r_mat_inv)\n",
        "    return mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "73e762d1",
      "metadata": {
        "id": "73e762d1"
      },
      "outputs": [],
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "ec661dc1",
      "metadata": {
        "id": "ec661dc1"
      },
      "outputs": [],
      "source": [
        "def load_data(path=\"/content/data/cora/\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1]) # one-hot encoding the labels\n",
        "\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    features = feature_normalize(features)\n",
        "\n",
        "    adj = adj_normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "5bce67b2",
      "metadata": {
        "id": "5bce67b2"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "da7e8928",
      "metadata": {
        "id": "da7e8928"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    def __init__(self, in_feat, out_feat, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_feat\n",
        "        self.out_features = out_feat\n",
        "        self.weight = Parameter(torch.FloatTensor(in_feat, out_feat))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_feat))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, inp, adj, max_neighbors=20):\n",
        "        # Apply random masking to the adjacency matrix\n",
        "        num_nodes = adj.shape[0]\n",
        "        mask = torch.rand(num_nodes, num_nodes).to(adj.device)\n",
        "        mask = mask < (max_neighbors / num_nodes)  # Mask only a subset of neighbors\n",
        "\n",
        "        # Apply the mask to the adjacency matrix\n",
        "        adj_masked = adj.to_dense() * mask\n",
        "\n",
        "        support = torch.mm(inp, self.weight)\n",
        "        output = torch.spmm(adj_masked, support)  # Use masked adjacency matrix\n",
        "\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "9b147458",
      "metadata": {
        "id": "9b147458"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gcn1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gcnn2 = GraphConvolution(nhid, nhid)\n",
        "        self.gcn2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj, max_neighbors=20):\n",
        "        x = F.relu(self.gcn1(x, adj, max_neighbors))  # Apply random neighborhood aggregation\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.relu(self.gcnn2(x, adj, max_neighbors))  # Apply random neighborhood aggregation\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gcn2(x, adj, max_neighbors)  # Apply random neighborhood aggregation\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "990494b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "990494b4",
        "outputId": "49b488ce-a01a-49db-f857-a5af52ca7801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "adj, features, labels, train_ids, val_ids, test_ids = load_data()\n",
        "\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=0.5)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=0.01, weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "e2542c4d",
      "metadata": {
        "id": "e2542c4d"
      },
      "outputs": [],
      "source": [
        "# If CUDA is available, move all data to GPU\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    train_ids.cuda()\n",
        "    val_ids.cuda()\n",
        "    test_ids.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "146eb197",
      "metadata": {
        "id": "146eb197"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[train_ids], labels[train_ids])\n",
        "    acc_train = accuracy(output[train_ids], labels[train_ids])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_val = F.nll_loss(output[val_ids], labels[val_ids])\n",
        "    acc_val = accuracy(output[val_ids], labels[val_ids])\n",
        "\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "b32be098",
      "metadata": {
        "id": "b32be098"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[test_ids], labels[test_ids])\n",
        "    acc_test = accuracy(output[test_ids], labels[test_ids])\n",
        "    _, predicted = torch.max(output[test_ids], 1)\n",
        "\n",
        "    precision = precision_score(labels[test_ids].cpu(), predicted.cpu(), average='macro', zero_division=1)\n",
        "    recall = recall_score(labels[test_ids].cpu(), predicted.cpu(), average='macro', zero_division=1)\n",
        "    f1 = f1_score(labels[test_ids].cpu(), predicted.cpu(), average='macro', zero_division=1)\n",
        "\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
        "          \"precision= {:.4f}\".format(precision),\n",
        "          \"recall= {:.4f}\".format(recall),\n",
        "          \"F1= {:.4f}\".format(f1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_total = time.time()\n",
        "\n",
        "for epoch in range(200):\n",
        "    train(epoch)\n",
        "\n",
        "print('Optimization finished...')\n",
        "print('Total time elapsed: {:.4f}s'.format(time.time() - t_total))\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyrDxbpU4fPV",
        "outputId": "45ca8058-0b05-4610-ef3b-f576fc303039"
      },
      "id": "wyrDxbpU4fPV",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9739 acc_train: 0.0786 loss_val: 1.9823 acc_val: 0.0833 time: 1.3068s\n",
            "Epoch: 0002 loss_train: 1.9699 acc_train: 0.0857 loss_val: 1.9775 acc_val: 0.0833 time: 0.9172s\n",
            "Epoch: 0003 loss_train: 1.9682 acc_train: 0.0714 loss_val: 1.9738 acc_val: 0.1567 time: 0.8363s\n",
            "Epoch: 0004 loss_train: 1.9600 acc_train: 0.2000 loss_val: 1.9695 acc_val: 0.1567 time: 0.6142s\n",
            "Epoch: 0005 loss_train: 1.9578 acc_train: 0.1929 loss_val: 1.9658 acc_val: 0.1567 time: 0.6202s\n",
            "Epoch: 0006 loss_train: 1.9536 acc_train: 0.2000 loss_val: 1.9619 acc_val: 0.1567 time: 0.6149s\n",
            "Epoch: 0007 loss_train: 1.9492 acc_train: 0.2000 loss_val: 1.9581 acc_val: 0.1567 time: 0.6464s\n",
            "Epoch: 0008 loss_train: 1.9461 acc_train: 0.2000 loss_val: 1.9539 acc_val: 0.1567 time: 0.6214s\n",
            "Epoch: 0009 loss_train: 1.9417 acc_train: 0.2000 loss_val: 1.9499 acc_val: 0.1567 time: 0.6444s\n",
            "Epoch: 0010 loss_train: 1.9392 acc_train: 0.2000 loss_val: 1.9469 acc_val: 0.1567 time: 0.6290s\n",
            "Epoch: 0011 loss_train: 1.9336 acc_train: 0.2000 loss_val: 1.9436 acc_val: 0.1567 time: 0.6834s\n",
            "Epoch: 0012 loss_train: 1.9318 acc_train: 0.2000 loss_val: 1.9408 acc_val: 0.1567 time: 0.6092s\n",
            "Epoch: 0013 loss_train: 1.9291 acc_train: 0.2000 loss_val: 1.9379 acc_val: 0.1567 time: 0.6321s\n",
            "Epoch: 0014 loss_train: 1.9264 acc_train: 0.2000 loss_val: 1.9333 acc_val: 0.1567 time: 0.6481s\n",
            "Epoch: 0015 loss_train: 1.9231 acc_train: 0.2000 loss_val: 1.9317 acc_val: 0.1567 time: 0.6169s\n",
            "Epoch: 0016 loss_train: 1.9207 acc_train: 0.2000 loss_val: 1.9286 acc_val: 0.1567 time: 0.6139s\n",
            "Epoch: 0017 loss_train: 1.9178 acc_train: 0.2000 loss_val: 1.9254 acc_val: 0.1567 time: 0.6295s\n",
            "Epoch: 0018 loss_train: 1.9140 acc_train: 0.2000 loss_val: 1.9227 acc_val: 0.1567 time: 0.5996s\n",
            "Epoch: 0019 loss_train: 1.9130 acc_train: 0.2000 loss_val: 1.9195 acc_val: 0.1567 time: 0.8725s\n",
            "Epoch: 0020 loss_train: 1.9076 acc_train: 0.2071 loss_val: 1.9170 acc_val: 0.1567 time: 1.0372s\n",
            "Epoch: 0021 loss_train: 1.9060 acc_train: 0.2000 loss_val: 1.9146 acc_val: 0.1567 time: 0.9837s\n",
            "Epoch: 0022 loss_train: 1.9035 acc_train: 0.2000 loss_val: 1.9115 acc_val: 0.1567 time: 0.7905s\n",
            "Epoch: 0023 loss_train: 1.9008 acc_train: 0.2000 loss_val: 1.9095 acc_val: 0.1567 time: 0.6242s\n",
            "Epoch: 0024 loss_train: 1.8988 acc_train: 0.2000 loss_val: 1.9058 acc_val: 0.1567 time: 0.6106s\n",
            "Epoch: 0025 loss_train: 1.8982 acc_train: 0.2000 loss_val: 1.9043 acc_val: 0.1567 time: 0.6716s\n",
            "Epoch: 0026 loss_train: 1.8941 acc_train: 0.2000 loss_val: 1.9015 acc_val: 0.1567 time: 0.6026s\n",
            "Epoch: 0027 loss_train: 1.8911 acc_train: 0.2000 loss_val: 1.8994 acc_val: 0.1567 time: 0.6231s\n",
            "Epoch: 0028 loss_train: 1.8872 acc_train: 0.2000 loss_val: 1.8967 acc_val: 0.1567 time: 0.6235s\n",
            "Epoch: 0029 loss_train: 1.8859 acc_train: 0.2000 loss_val: 1.8936 acc_val: 0.1567 time: 0.6156s\n",
            "Epoch: 0030 loss_train: 1.8848 acc_train: 0.2000 loss_val: 1.8915 acc_val: 0.1567 time: 0.6159s\n",
            "Epoch: 0031 loss_train: 1.8819 acc_train: 0.2000 loss_val: 1.8889 acc_val: 0.1567 time: 0.6079s\n",
            "Epoch: 0032 loss_train: 1.8807 acc_train: 0.2000 loss_val: 1.8864 acc_val: 0.1567 time: 0.6218s\n",
            "Epoch: 0033 loss_train: 1.8785 acc_train: 0.2000 loss_val: 1.8841 acc_val: 0.1567 time: 0.6107s\n",
            "Epoch: 0034 loss_train: 1.8769 acc_train: 0.1929 loss_val: 1.8811 acc_val: 0.1567 time: 0.6168s\n",
            "Epoch: 0035 loss_train: 1.8720 acc_train: 0.2071 loss_val: 1.8779 acc_val: 0.1633 time: 0.6356s\n",
            "Epoch: 0036 loss_train: 1.8727 acc_train: 0.2000 loss_val: 1.8763 acc_val: 0.1600 time: 0.5957s\n",
            "Epoch: 0037 loss_train: 1.8703 acc_train: 0.2000 loss_val: 1.8744 acc_val: 0.1567 time: 0.6275s\n",
            "Epoch: 0038 loss_train: 1.8690 acc_train: 0.1929 loss_val: 1.8717 acc_val: 0.1600 time: 0.8329s\n",
            "Epoch: 0039 loss_train: 1.8671 acc_train: 0.2000 loss_val: 1.8695 acc_val: 0.1533 time: 1.0492s\n",
            "Epoch: 0040 loss_train: 1.8626 acc_train: 0.2143 loss_val: 1.8678 acc_val: 0.1567 time: 0.9946s\n",
            "Epoch: 0041 loss_train: 1.8640 acc_train: 0.2000 loss_val: 1.8648 acc_val: 0.1567 time: 0.8910s\n",
            "Epoch: 0042 loss_train: 1.8637 acc_train: 0.2000 loss_val: 1.8641 acc_val: 0.1533 time: 0.6198s\n",
            "Epoch: 0043 loss_train: 1.8584 acc_train: 0.2071 loss_val: 1.8595 acc_val: 0.1600 time: 0.6200s\n",
            "Epoch: 0044 loss_train: 1.8585 acc_train: 0.2000 loss_val: 1.8594 acc_val: 0.1533 time: 0.6076s\n",
            "Epoch: 0045 loss_train: 1.8564 acc_train: 0.2000 loss_val: 1.8579 acc_val: 0.1567 time: 0.6227s\n",
            "Epoch: 0046 loss_train: 1.8553 acc_train: 0.2071 loss_val: 1.8547 acc_val: 0.1600 time: 0.6255s\n",
            "Epoch: 0047 loss_train: 1.8532 acc_train: 0.2071 loss_val: 1.8532 acc_val: 0.1567 time: 0.6161s\n",
            "Epoch: 0048 loss_train: 1.8532 acc_train: 0.1857 loss_val: 1.8512 acc_val: 0.3500 time: 0.6165s\n",
            "Epoch: 0049 loss_train: 1.8510 acc_train: 0.2929 loss_val: 1.8506 acc_val: 0.3500 time: 0.6125s\n",
            "Epoch: 0050 loss_train: 1.8508 acc_train: 0.2929 loss_val: 1.8475 acc_val: 0.3500 time: 0.6100s\n",
            "Epoch: 0051 loss_train: 1.8480 acc_train: 0.2929 loss_val: 1.8464 acc_val: 0.3500 time: 0.6505s\n",
            "Epoch: 0052 loss_train: 1.8480 acc_train: 0.2929 loss_val: 1.8446 acc_val: 0.3500 time: 0.6098s\n",
            "Epoch: 0053 loss_train: 1.8473 acc_train: 0.2929 loss_val: 1.8423 acc_val: 0.3500 time: 0.6125s\n",
            "Epoch: 0054 loss_train: 1.8456 acc_train: 0.2929 loss_val: 1.8411 acc_val: 0.3500 time: 0.6133s\n",
            "Epoch: 0055 loss_train: 1.8452 acc_train: 0.2929 loss_val: 1.8402 acc_val: 0.3500 time: 0.6197s\n",
            "Epoch: 0056 loss_train: 1.8437 acc_train: 0.2929 loss_val: 1.8385 acc_val: 0.3500 time: 0.6227s\n",
            "Epoch: 0057 loss_train: 1.8422 acc_train: 0.2929 loss_val: 1.8373 acc_val: 0.3500 time: 0.7136s\n",
            "Epoch: 0058 loss_train: 1.8394 acc_train: 0.2929 loss_val: 1.8364 acc_val: 0.3500 time: 1.0029s\n",
            "Epoch: 0059 loss_train: 1.8410 acc_train: 0.2929 loss_val: 1.8333 acc_val: 0.3500 time: 0.9936s\n",
            "Epoch: 0060 loss_train: 1.8395 acc_train: 0.2929 loss_val: 1.8325 acc_val: 0.3500 time: 0.9656s\n",
            "Epoch: 0061 loss_train: 1.8364 acc_train: 0.2929 loss_val: 1.8295 acc_val: 0.3500 time: 0.6512s\n",
            "Epoch: 0062 loss_train: 1.8380 acc_train: 0.2929 loss_val: 1.8294 acc_val: 0.3500 time: 0.6068s\n",
            "Epoch: 0063 loss_train: 1.8359 acc_train: 0.2929 loss_val: 1.8305 acc_val: 0.3500 time: 0.6279s\n",
            "Epoch: 0064 loss_train: 1.8384 acc_train: 0.2929 loss_val: 1.8281 acc_val: 0.3500 time: 0.6072s\n",
            "Epoch: 0065 loss_train: 1.8359 acc_train: 0.2929 loss_val: 1.8268 acc_val: 0.3500 time: 0.6181s\n",
            "Epoch: 0066 loss_train: 1.8339 acc_train: 0.2929 loss_val: 1.8264 acc_val: 0.3500 time: 0.6073s\n",
            "Epoch: 0067 loss_train: 1.8340 acc_train: 0.2929 loss_val: 1.8244 acc_val: 0.3500 time: 0.6146s\n",
            "Epoch: 0068 loss_train: 1.8329 acc_train: 0.2929 loss_val: 1.8240 acc_val: 0.3500 time: 0.5950s\n",
            "Epoch: 0069 loss_train: 1.8315 acc_train: 0.2929 loss_val: 1.8235 acc_val: 0.3500 time: 0.6585s\n",
            "Epoch: 0070 loss_train: 1.8309 acc_train: 0.2929 loss_val: 1.8223 acc_val: 0.3500 time: 0.6076s\n",
            "Epoch: 0071 loss_train: 1.8303 acc_train: 0.2929 loss_val: 1.8214 acc_val: 0.3500 time: 0.6287s\n",
            "Epoch: 0072 loss_train: 1.8282 acc_train: 0.2929 loss_val: 1.8214 acc_val: 0.3500 time: 0.6185s\n",
            "Epoch: 0073 loss_train: 1.8285 acc_train: 0.2929 loss_val: 1.8214 acc_val: 0.3500 time: 0.6190s\n",
            "Epoch: 0074 loss_train: 1.8279 acc_train: 0.2929 loss_val: 1.8189 acc_val: 0.3500 time: 0.6023s\n",
            "Epoch: 0075 loss_train: 1.8306 acc_train: 0.2929 loss_val: 1.8199 acc_val: 0.3500 time: 0.6063s\n",
            "Epoch: 0076 loss_train: 1.8278 acc_train: 0.2929 loss_val: 1.8178 acc_val: 0.3500 time: 0.6065s\n",
            "Epoch: 0077 loss_train: 1.8228 acc_train: 0.2929 loss_val: 1.8167 acc_val: 0.3500 time: 0.9285s\n",
            "Epoch: 0078 loss_train: 1.8257 acc_train: 0.2929 loss_val: 1.8175 acc_val: 0.3500 time: 1.0087s\n",
            "Epoch: 0079 loss_train: 1.8282 acc_train: 0.2929 loss_val: 1.8172 acc_val: 0.3500 time: 0.9761s\n",
            "Epoch: 0080 loss_train: 1.8237 acc_train: 0.2929 loss_val: 1.8169 acc_val: 0.3500 time: 0.8236s\n",
            "Epoch: 0081 loss_train: 1.8253 acc_train: 0.2929 loss_val: 1.8139 acc_val: 0.3500 time: 0.6242s\n",
            "Epoch: 0082 loss_train: 1.8247 acc_train: 0.2929 loss_val: 1.8135 acc_val: 0.3500 time: 0.6174s\n",
            "Epoch: 0083 loss_train: 1.8239 acc_train: 0.2929 loss_val: 1.8144 acc_val: 0.3500 time: 0.6114s\n",
            "Epoch: 0084 loss_train: 1.8246 acc_train: 0.2929 loss_val: 1.8138 acc_val: 0.3500 time: 0.6249s\n",
            "Epoch: 0085 loss_train: 1.8249 acc_train: 0.2929 loss_val: 1.8112 acc_val: 0.3500 time: 0.7070s\n",
            "Epoch: 0086 loss_train: 1.8247 acc_train: 0.2929 loss_val: 1.8104 acc_val: 0.3500 time: 0.6830s\n",
            "Epoch: 0087 loss_train: 1.8225 acc_train: 0.2929 loss_val: 1.8120 acc_val: 0.3500 time: 0.6105s\n",
            "Epoch: 0088 loss_train: 1.8219 acc_train: 0.2929 loss_val: 1.8129 acc_val: 0.3500 time: 0.6161s\n",
            "Epoch: 0089 loss_train: 1.8244 acc_train: 0.2929 loss_val: 1.8106 acc_val: 0.3500 time: 0.6130s\n",
            "Epoch: 0090 loss_train: 1.8200 acc_train: 0.2929 loss_val: 1.8111 acc_val: 0.3500 time: 0.6019s\n",
            "Epoch: 0091 loss_train: 1.8224 acc_train: 0.2929 loss_val: 1.8098 acc_val: 0.3500 time: 0.6055s\n",
            "Epoch: 0092 loss_train: 1.8191 acc_train: 0.2929 loss_val: 1.8116 acc_val: 0.3500 time: 0.6070s\n",
            "Epoch: 0093 loss_train: 1.8211 acc_train: 0.2929 loss_val: 1.8100 acc_val: 0.3500 time: 0.6202s\n",
            "Epoch: 0094 loss_train: 1.8208 acc_train: 0.2929 loss_val: 1.8106 acc_val: 0.3500 time: 0.6195s\n",
            "Epoch: 0095 loss_train: 1.8208 acc_train: 0.2929 loss_val: 1.8099 acc_val: 0.3500 time: 0.6669s\n",
            "Epoch: 0096 loss_train: 1.8193 acc_train: 0.2929 loss_val: 1.8087 acc_val: 0.3500 time: 0.8474s\n",
            "Epoch: 0097 loss_train: 1.8147 acc_train: 0.2929 loss_val: 1.8078 acc_val: 0.3500 time: 1.0125s\n",
            "Epoch: 0098 loss_train: 1.8199 acc_train: 0.2929 loss_val: 1.8084 acc_val: 0.3500 time: 0.9633s\n",
            "Epoch: 0099 loss_train: 1.8198 acc_train: 0.2929 loss_val: 1.8063 acc_val: 0.3500 time: 0.7691s\n",
            "Epoch: 0100 loss_train: 1.8213 acc_train: 0.2929 loss_val: 1.8066 acc_val: 0.3500 time: 0.6094s\n",
            "Epoch: 0101 loss_train: 1.8199 acc_train: 0.2929 loss_val: 1.8069 acc_val: 0.3500 time: 0.6088s\n",
            "Epoch: 0102 loss_train: 1.8203 acc_train: 0.2929 loss_val: 1.8100 acc_val: 0.3500 time: 0.6092s\n",
            "Epoch: 0103 loss_train: 1.8129 acc_train: 0.2929 loss_val: 1.8076 acc_val: 0.3500 time: 0.6099s\n",
            "Epoch: 0104 loss_train: 1.8201 acc_train: 0.2929 loss_val: 1.8076 acc_val: 0.3500 time: 0.6190s\n",
            "Epoch: 0105 loss_train: 1.8242 acc_train: 0.2929 loss_val: 1.8059 acc_val: 0.3500 time: 0.6042s\n",
            "Epoch: 0106 loss_train: 1.8193 acc_train: 0.2929 loss_val: 1.8083 acc_val: 0.3500 time: 0.6112s\n",
            "Epoch: 0107 loss_train: 1.8218 acc_train: 0.2929 loss_val: 1.8057 acc_val: 0.3500 time: 0.6389s\n",
            "Epoch: 0108 loss_train: 1.8178 acc_train: 0.2929 loss_val: 1.8070 acc_val: 0.3500 time: 0.6176s\n",
            "Epoch: 0109 loss_train: 1.8215 acc_train: 0.2929 loss_val: 1.8065 acc_val: 0.3500 time: 0.6305s\n",
            "Epoch: 0110 loss_train: 1.8198 acc_train: 0.2929 loss_val: 1.8060 acc_val: 0.3500 time: 0.6182s\n",
            "Epoch: 0111 loss_train: 1.8211 acc_train: 0.2929 loss_val: 1.8065 acc_val: 0.3500 time: 0.6210s\n",
            "Epoch: 0112 loss_train: 1.8159 acc_train: 0.2929 loss_val: 1.8049 acc_val: 0.3500 time: 0.6149s\n",
            "Epoch: 0113 loss_train: 1.8174 acc_train: 0.2929 loss_val: 1.8059 acc_val: 0.3500 time: 0.6220s\n",
            "Epoch: 0114 loss_train: 1.8180 acc_train: 0.2929 loss_val: 1.8061 acc_val: 0.3500 time: 0.6110s\n",
            "Epoch: 0115 loss_train: 1.8184 acc_train: 0.2929 loss_val: 1.8059 acc_val: 0.3500 time: 0.8010s\n",
            "Epoch: 0116 loss_train: 1.8181 acc_train: 0.2929 loss_val: 1.8051 acc_val: 0.3500 time: 0.9847s\n",
            "Epoch: 0117 loss_train: 1.8196 acc_train: 0.2929 loss_val: 1.8052 acc_val: 0.3500 time: 0.9796s\n",
            "Epoch: 0118 loss_train: 1.8182 acc_train: 0.2929 loss_val: 1.8031 acc_val: 0.3500 time: 0.9604s\n",
            "Epoch: 0119 loss_train: 1.8162 acc_train: 0.2929 loss_val: 1.8033 acc_val: 0.3500 time: 0.6212s\n",
            "Epoch: 0120 loss_train: 1.8175 acc_train: 0.2929 loss_val: 1.8047 acc_val: 0.3500 time: 0.6366s\n",
            "Epoch: 0121 loss_train: 1.8175 acc_train: 0.2929 loss_val: 1.8040 acc_val: 0.3500 time: 0.6246s\n",
            "Epoch: 0122 loss_train: 1.8180 acc_train: 0.2929 loss_val: 1.8030 acc_val: 0.3500 time: 0.6083s\n",
            "Epoch: 0123 loss_train: 1.8161 acc_train: 0.2929 loss_val: 1.8051 acc_val: 0.3500 time: 0.6288s\n",
            "Epoch: 0124 loss_train: 1.8172 acc_train: 0.2929 loss_val: 1.8047 acc_val: 0.3500 time: 0.6246s\n",
            "Epoch: 0125 loss_train: 1.8173 acc_train: 0.2929 loss_val: 1.8047 acc_val: 0.3500 time: 0.5961s\n",
            "Epoch: 0126 loss_train: 1.8170 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.6160s\n",
            "Epoch: 0127 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8038 acc_val: 0.3500 time: 0.6032s\n",
            "Epoch: 0128 loss_train: 1.8165 acc_train: 0.2929 loss_val: 1.8046 acc_val: 0.3500 time: 0.6677s\n",
            "Epoch: 0129 loss_train: 1.8172 acc_train: 0.2929 loss_val: 1.8035 acc_val: 0.3500 time: 0.6986s\n",
            "Epoch: 0130 loss_train: 1.8170 acc_train: 0.2929 loss_val: 1.8030 acc_val: 0.3500 time: 0.6564s\n",
            "Epoch: 0131 loss_train: 1.8170 acc_train: 0.2929 loss_val: 1.8037 acc_val: 0.3500 time: 0.6350s\n",
            "Epoch: 0132 loss_train: 1.8160 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.6321s\n",
            "Epoch: 0133 loss_train: 1.8178 acc_train: 0.2929 loss_val: 1.8043 acc_val: 0.3500 time: 0.6126s\n",
            "Epoch: 0134 loss_train: 1.8153 acc_train: 0.2929 loss_val: 1.8030 acc_val: 0.3500 time: 0.7680s\n",
            "Epoch: 0135 loss_train: 1.8168 acc_train: 0.2929 loss_val: 1.8037 acc_val: 0.3500 time: 1.0071s\n",
            "Epoch: 0136 loss_train: 1.8185 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.9892s\n",
            "Epoch: 0137 loss_train: 1.8183 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 1.0417s\n",
            "Epoch: 0138 loss_train: 1.8177 acc_train: 0.2929 loss_val: 1.8016 acc_val: 0.3500 time: 0.6152s\n",
            "Epoch: 0139 loss_train: 1.8162 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.6368s\n",
            "Epoch: 0140 loss_train: 1.8176 acc_train: 0.2929 loss_val: 1.8029 acc_val: 0.3500 time: 0.6328s\n",
            "Epoch: 0141 loss_train: 1.8165 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.6364s\n",
            "Epoch: 0142 loss_train: 1.8181 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.6427s\n",
            "Epoch: 0143 loss_train: 1.8152 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.6303s\n",
            "Epoch: 0144 loss_train: 1.8161 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.6246s\n",
            "Epoch: 0145 loss_train: 1.8183 acc_train: 0.2929 loss_val: 1.8038 acc_val: 0.3500 time: 0.6243s\n",
            "Epoch: 0146 loss_train: 1.8174 acc_train: 0.2929 loss_val: 1.8032 acc_val: 0.3500 time: 0.6161s\n",
            "Epoch: 0147 loss_train: 1.8161 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.6240s\n",
            "Epoch: 0148 loss_train: 1.8163 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.6345s\n",
            "Epoch: 0149 loss_train: 1.8166 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.6119s\n",
            "Epoch: 0150 loss_train: 1.8161 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.6376s\n",
            "Epoch: 0151 loss_train: 1.8175 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.6104s\n",
            "Epoch: 0152 loss_train: 1.8169 acc_train: 0.2929 loss_val: 1.8031 acc_val: 0.3500 time: 0.6159s\n",
            "Epoch: 0153 loss_train: 1.8147 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.7369s\n",
            "Epoch: 0154 loss_train: 1.8165 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 1.0583s\n",
            "Epoch: 0155 loss_train: 1.8160 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.9773s\n",
            "Epoch: 0156 loss_train: 1.8162 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.9663s\n",
            "Epoch: 0157 loss_train: 1.8163 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.6565s\n",
            "Epoch: 0158 loss_train: 1.8154 acc_train: 0.2929 loss_val: 1.8031 acc_val: 0.3500 time: 0.6090s\n",
            "Epoch: 0159 loss_train: 1.8156 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.6145s\n",
            "Epoch: 0160 loss_train: 1.8159 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.5903s\n",
            "Epoch: 0161 loss_train: 1.8179 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.6214s\n",
            "Epoch: 0162 loss_train: 1.8161 acc_train: 0.2929 loss_val: 1.8021 acc_val: 0.3500 time: 0.5958s\n",
            "Epoch: 0163 loss_train: 1.8152 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.6267s\n",
            "Epoch: 0164 loss_train: 1.8176 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.6145s\n",
            "Epoch: 0165 loss_train: 1.8157 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.6185s\n",
            "Epoch: 0166 loss_train: 1.8170 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.6345s\n",
            "Epoch: 0167 loss_train: 1.8155 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.6225s\n",
            "Epoch: 0168 loss_train: 1.8159 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.6315s\n",
            "Epoch: 0169 loss_train: 1.8162 acc_train: 0.2929 loss_val: 1.8019 acc_val: 0.3500 time: 0.6282s\n",
            "Epoch: 0170 loss_train: 1.8159 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.5992s\n",
            "Epoch: 0171 loss_train: 1.8168 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.6245s\n",
            "Epoch: 0172 loss_train: 1.8161 acc_train: 0.2929 loss_val: 1.8028 acc_val: 0.3500 time: 0.6211s\n",
            "Epoch: 0173 loss_train: 1.8155 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.9674s\n",
            "Epoch: 0174 loss_train: 1.8150 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.9902s\n",
            "Epoch: 0175 loss_train: 1.8165 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.9750s\n",
            "Epoch: 0176 loss_train: 1.8153 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.7723s\n",
            "Epoch: 0177 loss_train: 1.8154 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.5941s\n",
            "Epoch: 0178 loss_train: 1.8174 acc_train: 0.2929 loss_val: 1.8023 acc_val: 0.3500 time: 0.6259s\n",
            "Epoch: 0179 loss_train: 1.8162 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.6108s\n",
            "Epoch: 0180 loss_train: 1.8156 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.6340s\n",
            "Epoch: 0181 loss_train: 1.8161 acc_train: 0.2929 loss_val: 1.8026 acc_val: 0.3500 time: 0.5940s\n",
            "Epoch: 0182 loss_train: 1.8159 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.6276s\n",
            "Epoch: 0183 loss_train: 1.8146 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.6038s\n",
            "Epoch: 0184 loss_train: 1.8160 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.6233s\n",
            "Epoch: 0185 loss_train: 1.8155 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.6051s\n",
            "Epoch: 0186 loss_train: 1.8159 acc_train: 0.2929 loss_val: 1.8021 acc_val: 0.3500 time: 0.6102s\n",
            "Epoch: 0187 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.6005s\n",
            "Epoch: 0188 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8027 acc_val: 0.3500 time: 0.6120s\n",
            "Epoch: 0189 loss_train: 1.8167 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.6307s\n",
            "Epoch: 0190 loss_train: 1.8151 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.6353s\n",
            "Epoch: 0191 loss_train: 1.8159 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.6115s\n",
            "Epoch: 0192 loss_train: 1.8166 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.8357s\n",
            "Epoch: 0193 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8021 acc_val: 0.3500 time: 1.0325s\n",
            "Epoch: 0194 loss_train: 1.8157 acc_train: 0.2929 loss_val: 1.8016 acc_val: 0.3500 time: 0.9810s\n",
            "Epoch: 0195 loss_train: 1.8155 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.7747s\n",
            "Epoch: 0196 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8022 acc_val: 0.3500 time: 0.5902s\n",
            "Epoch: 0197 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.6145s\n",
            "Epoch: 0198 loss_train: 1.8158 acc_train: 0.2929 loss_val: 1.8021 acc_val: 0.3500 time: 0.6121s\n",
            "Epoch: 0199 loss_train: 1.8156 acc_train: 0.2929 loss_val: 1.8018 acc_val: 0.3500 time: 0.6101s\n",
            "Epoch: 0200 loss_train: 1.8151 acc_train: 0.2929 loss_val: 1.8020 acc_val: 0.3500 time: 0.6193s\n",
            "Optimization finished...\n",
            "Total time elapsed: 139.9348s\n",
            "Test set results: loss= 1.8877 accuracy= 0.3090 precision= 0.9013 recall= 0.1429 F1= 0.0674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af85139",
      "metadata": {
        "id": "1af85139"
      },
      "source": [
        "###"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}